## 기본 용어
+ 코퍼스(corpus): 말뭉치, 자연어 처리 연구를 위해 특정한 목적에서 표본을 추출한 자연어 집합
+ 토큰(token): 자연어 처리를 위한 문서는 작은 단위로 나뉘어야 하는데, 이때 문서(문장)를 나누는 단위
+ 토큰화(tokenization): 텍스트를 문장이나 단어(토큰)로 분리하는 것
	+ 한국어 tokenization은 주로 형태소 분석으로 진행: hannanum, Kkma, Komoran, Okt
+ 불용어(stop words): 문장에 자주 등장하지만 분석에는 의미가 없는 용어들(조사, 접미사 등)
+ 어간 추출(stemming): 어간(stem)을 추출하는 작업. 
	+ 형태학적 분석을 단순화한 버전이며, 정해진 규칙에 따라 단순히 단어의 접미사나 어미를 제거하여 단어의 원형을 파악하는 과정
	+ 어간 추출 후에 나오는 결과는 사전에 존재하지 않는 단어일 수도 있다
+ 표제어 추출(lemmatizing): 단어의 원형인 표제어(lemma)를 추출하는 과정으로, 어간 추출과는 달리 단어의 원래 형태를 유지한다
## 활용 분야
+ 이메일 필터
+ 스마트 어시스턴트
+ 검색 결과
+ 자동완성
+ 번역
+ ChatGPT
### Sentiment Analysis
감성 분석으로, 자연어 처리(NLP) 분야에서 텍스트의 감성과 긍정/부정/중립과 같은 감정 상태를 자동으로 감지하고 분류하는 기술. 온라인 평론, 제품/서비스 리뷰, 소셜 미디어 포스트 및 커머스에서 고객의 반응과 의견을 이해하는 데 유용한 기술
## 자연어 처리 과정
1. preprocessing
	1. 토큰화
	2. 불용어 제거
	3. 어간 or 표제어 추출
2. embedding(word to vector)
3. model 처리
## Word Embedding
자연어 처리에서 사람이 사용하는 언어(자연어)를 컴퓨터가 이해할 수 있는 언어(숫자) 형태인 벡터(vector)로 변환한 결과 혹은 일련의 과정을 의미. 

자연어를 딥러닝 모델의 입력으로 사용하기 위해 거쳐야되는 단계. LSA, Word2Vec, GloVe, FastText 등이 존재
### one-hot encoding
단어 N개를 각각 N차원의 벡터로 표현하는 방식으로, 단어가 포함되어 있는 위치에 1을 넣고 나머지에는 0을 채운다. 아래와 같은 문제가 존재
+ 단어끼리 관계성(유의어, 반의어) 없이 서로 독립적인 관계가 된다.
+ 하나의 단어를 표현하는데 corpus에 존재하는 단어 수만큼 차원이 필요
### TF-IDF
Term Frequency - Inverse Document Frequency. 정보 검색에서 자주 사용되는 문서의 유사성을 측정하는 방법. 문서 내의 각 단어의 빈도수에 따라 가중치를 부여하는 방법. 

TF-IDF 값은 특정 문서에서의 특정 단어의 빈도수(TF)와 전체 문서에서 해당 단어의 역빈도수(IDF)를 곱한 값. 즉, 특정 문서에서 자주 나타나는 단어에 대해서는 높은 가중치를 부여하고, 모든 문서에서 공통적으로 자주 나타나는 단어에 대해서는 낮은 가중치를 부여한다.

![500](Pasted%20image%2020240615221020.png)

다음과 같은 문제들이 존재
+ 단어의 의미를 고려하지 않는다: 단어의 의미나 문맥을 고려하지 않아 동음이의어나 유의어를 구분하지 못함 
+ 희소성 문제(sparsity problem): 대부분의 문서에서는 모든 문서에 등장하는 전체 단어 중 일부 단어만이 등장하고, 대부분의 단어가 등장하지 않으면 희소벡터가 생성된다. 이러한 문제는 계산 복잡도를 늘리고, 정확도를 낮춘다.
+ 단어의 중요도가 과대평가될 수 있다: 일부 문서에서 특정 단어가 자주 등장하는 경우 해당 단어가 문서 전반적으로 중요한 역할을 한다는 결론을 내어 정확도를 낮출 수 있음
### Word2Vec
2013년 구글에서 공개된 단어를 벡터로 표현하는 대표적인 기법. CBOW & Skip-gram을 이용

![500](Pasted%20image%2020240615221405.png)
+ CBOW(Continuous Bag of Words): 주변에 있는 단어들을 입력으로 중간에 있는 단어를 예측하는 방법
	+ 중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지 결정해야 한다. 아래와 같은 과정으로 lookup table이 완성된다.
	+ Lookup table은 데이터베이스나 표와 같은 데이터 구조에서 키와 값을 저장하는 테이블
![500](Pasted%20image%2020240615221637.png)
![500](Pasted%20image%2020240615221645.png)
![500](Pasted%20image%2020240615221733.png)
+ Skip-gram: 중심 단어를 기준으로 주변 단어를 예측하는 모델. 전반적으로 CBOW보다 성능이 좋음
![500](Pasted%20image%2020240615221825.png)
### FastText
2016년 페이스북에서 공개한 텍스트를 단어 단위가 아니라 subword(단어 내의 작은 단위) 단위로 표현하는 워드 임베딩 방법. 특히 희소한 단어나 이모티콘, 철자 오류와 같은 다양한 텍스트 상황에서 강력한 성능을 발휘한다.

각 단어를 n-gram 구성으로 취급한다.
+ 만약 'apple'에 n = 3인 경우 아래와 같이 단어가 세분화된다.
+ \<ap, app, ppl, ple, le\>, \<apple\>
+ 단어 apple의 벡터값은 나눠진(세부) 단어들의 벡터 합으로 표현된다.

OOV(Out of Vocabulary)에 대한 대응이 가능하고, 빈도가 적었던 단어에 대한 대응이 가능하다. 희귀 단어라도 n-gram이 다른 단어의 n-gram과 겹치는 경우라면 Word2Vec보다 상대적으로 높은 정확도의 임베딩 벡터를 얻을 수 있음
## Attention
+ Seq2seq(sequence to sequence) 모델은 고정된 크기의 context 벡터에 모든 정보를 압축해야 하므로 정보 손실이 발생. Gradient vanishing 문제 발생

![500](Pasted%20image%2020240616145617.png)

attention은 입력 시퀀스의 특정 부분에 더 많은 가중치를 부여하여 모델이 더욱 정확한 예측을 수행할 수 있도록 하는 기법. 기존 seq2seq 모델은 입력 시퀀스의 모든 정보를 고정된 크기의 벡터에 담을 수 없기 때문에 입력 시퀀스의 특정 부분에 더 많은 가중치를 부여하여 더 많은 정보를 반영하기 위한 방법

![500](Pasted%20image%2020240616150012.png)
## Transformer
2017년 구글이 발표한 논문인 "Attention is all you need"에서 나온 모델로 기존의 seq2seq의 구조인 인코더-디코도를 따르면서도, attention으로만 구현한 모델

![500](Pasted%20image%2020240616150146.png)
### Self-Attention
주어진 입력 시퀀스 내에서 각 위초의 원소들 간의 상대적 중요도를 계산하는 메커니즘. 이를 통해 입력 시퀀스의 각 위치가 다른 위치들과 얼마나 관련되어 있는지를 나타낼 수 있다.

![500](Pasted%20image%2020240616150311.png)
+ Query(Q): 현재 위치의 임베딩 벡터. 다른 위치의 key 벡터들과의 관련성을 측정하는 데 사용 (위치 + 연관성)
+ Key(K): 입력 시퀀스의 다른 위치들의 임베딩 벡터. query 벡터와의 관련성을 계산하는 데 사용 (현재 벡터 외 다른 위치들의 값)
+ Value(V): 입력 시퀀스의 임베딩 벡터로, 정보를 나타냄. attention score를 기반으로 가중평균을 계산하는 데 사용 (단어의 의미)

각 단어 벡터들로부터 Q, K, V 벡터를 얻는 작업을 수행한다. 

![500](Pasted%20image%2020240616150742.png)

'I'에 대한 Q 벡터가 모든 K벡터에 대해 어텐션 스코어를 구한다. 기본적으로 벡터의 크기에 비례해 어텐션 스코어를 구한다. 기본적으로 벡터의 크기에 비례해 어텐션 스코어도 증가한다. 이때 학습의 안정성을 위해 $\sqrt{d_k}$ 로 나눠준다(scaled dot-product attention)

![500](Pasted%20image%2020240616150947.png)

softmax 함수를 사용하여 attention distribution을 구하고, 각 v벡터와 가중합하여 attention value를 구한다. 이를 단어 'I'에 대한 어텐션 값 또는 단어 'I'에 대한 context vector라고도 할 수 있다

![500](Pasted%20image%2020240616151125.png)

연산을 행렬연산으로 수행해 병렬처리해 연산량을 증가시키고, 속도 또한 증가시킬 수 있음
### Multi-Head Attention
한 번의 attention보다 여러 번의 attention을 사용하는 것이 더 효과적이다. 

![500](Pasted%20image%2020240616151813.png)

이때 input 값과 output의 크기가 같아야 같은 구조를 n번 반복할 수 있다

![500](Pasted%20image%2020240616151956.png)
### Add & Normalize
![500](Pasted%20image%2020240616152106.png)
### Embedding layer
전체 학습 과정에서 lookup table도 학습이 진행되는데, 이 lookup table은 embadding에서 사용된다. 
#### Byte Pair Encoding(BPE)
기본적으로 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식을 수행. 
1. 초기 구성은 글자 단위로 분리된 상태로 구성한다.
```python
# dictionary
l o w: 5, l o w e r: 2, n e w e s t: 6, w i d e s t: 3 # 빈도 계산
# vocabulary
l, o, w, e, r, n, s, t, i, d # 글자 분리
```
2. 딕셔너리를 참고를 하였을 때 빈도수가 9로 가장 높은 (e, s)의 쌍을 es로 통합
```python
# dictionary update
l o w: 5,
l o w e r: 2,
n e w es t: 6,
w i d es t: 3
# vocabulary update
l, o, w, e, r, n, s, t, i, d, es # 결합
```
3. 이를 특정 횟수만큼 반복
```python
# 10회 반복했을 때의 결과
# dictionay update
low: 5,
low e r: 2,
newest: 6,
widest: 3
# vocabulary update
l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest # 빈도수에 따라 차례대로 업데이트
```

이러한 방법은 여러 단어를 분해해 임베딩을 수행해 새로운 단어가 등장해도 대응이 가능하다. 
#### Positional Encoding
Transformer는 입력된 데이터를 한 번에 병렬로 처리해서 속도가 빠르다. 하지만 입력 순서에 대한 정보를 보장하지 않는 문제가 존재한다. 하지만 문장에서 단어 순서는 중요한 정보를 포함하고 있기 때문에 이러한 순서를 나타내기 위해 이 방법을 이용한다

단어의 위치를 나타내는 방법은 다양하지만, 다음과 같은 조건들이 지켜져야 한다. 
+ 각 위치 값은 데이터의 길이나 입력 값에 관계 없이 동일한 위치 값을 가져야 한다.
+ 각 위치에 대해 고유한 인코딩 값을 가져야 한다. (인코딩 값은 위치에 따라 서로 다른 값을 가져야 함)
+ 모든 위치 값은 입력 값에 비해 너무 크면 안된다. (데이터의 의미가 상대적으로 작아지게 된다)
+ 위치 차이에 의한 인코딩 값의 차이를 거리로 이용할 수 있어야 한다. 
	+ 간격이 동일하다면 위치와 관계없이 거리는 동일해야 한다.

인코딩 방법
1. 단순히 정수를 indexing할 경우 문장이 길어지면 숫자가 과도하게 커짐
2. 숫자가 커지지 않게 위해 첫 단어를 0, 마지막 단어를 1로 지정. 이 경우 문장의 길이에 따라 같은 위치임에도 다른 인코딩 값을 가지게 된다.
3. 이진수를 사용할 경우 위치 사이의 거리가 다르다. 
4. sin 함수를 이용하는 경우 주기함수이므로 길이에 제약 받지 않는다. 
	+ 주기함수이므로, 문장이 길어지면 동일한 인코딩 값이 발생할 수 있음
5. sin & cos 함수를 함계 사용해 동일한 인코딩 값이 발생하지 않도록 조정

![500](Pasted%20image%2020240616153540.png)

sin & cos 함수를 이용한 임베딩을 한다면 위치의 상대적 변화는 곧 삼각함수의 rotation matrix로 나타낼 수 있다.

![500](Pasted%20image%2020240616153743.png)

positional encoidng 시각화를 한다면 다음과 같은 예시가 나온다.

![500](Pasted%20image%2020240616153839.png)
### Masked Multi-Head Attention
디코더에만 사용되는 기법. Transformer는 교사 강요(teacher forcing)을 사용하여 훈련되는 형태이므로, encoder-decoder 형태로 구성

디코더에서 문장 행렬을 한번에 받을 경우 정답을 알고 예측하는 꼴이 되므로, 현재 시점 이후의 문장은 masking을 해야 한다. 

![500](Pasted%20image%2020240616194102.png)
### BERT&GPT
#### Generative Pre-trained Transformer(GPT)
문장을 순차적으로 학습하는 기법으로, transformer 구조에서 decoder 부분만 사용한 형태. 이전 단어를 기반으로 다음 단어를 예측
#### Bidirectional Encoder Representations from Transformers(BERT)
양방향 단어를 보고 빈칸 단어 즉, 중간 단어를 예측하는 형태로, transformer 구조에서 encoder만 사용한다.
### ViT
Transformer를 이미지 처리에 적용한 형태의 모델. Transformer encoder를 사용해 이미지를 학습시킴